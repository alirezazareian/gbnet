{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "codebase = '../../'\n",
    "sys.path.append(codebase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'exp_045_rep'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time as time_time\n",
    "import numpy as np\n",
    "# from torch import optim\n",
    "from apex import amp\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "write = tqdm.write\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from config import ModelConfig, BOX_SCALE, IM_SCALE\n",
    "from torch.nn import functional as F\n",
    "from lib.pytorch_misc import optimistic_restore, de_chunkize, clip_grad_norm\n",
    "from lib.evaluation.sg_eval import BasicSceneGraphEvaluator, calculate_mR_from_evaluator_list, eval_entry\n",
    "from lib.pytorch_misc import print_para\n",
    "from dataloaders.visual_genome import VGDataLoader, VG\n",
    "\n",
    "from lib.my_model_24 import KERN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~ Hyperparameters used: ~~~~~~~\n",
      "ckpt : checkpoints/vgdet/vgrel-12.tar\n",
      "save_dir : checkpoints/kern_predcls/exp_045_rep\n",
      "num_gpus : 1\n",
      "num_workers : 1\n",
      "lr : 0.0001\n",
      "batch_size : 3\n",
      "val_size : 5000\n",
      "l2 : 0.0001\n",
      "adamwd : 0.0\n",
      "clip : 5.0\n",
      "print_interval : 1000\n",
      "mode : predcls\n",
      "cache : \n",
      "adam : True\n",
      "test : False\n",
      "num_epochs : 50\n",
      "use_resnet : False\n",
      "use_proposals : False\n",
      "pooling_dim : 4096\n",
      "use_ggnn_obj : False\n",
      "ggnn_obj_time_step_num : 3\n",
      "ggnn_obj_hidden_dim : 512\n",
      "ggnn_obj_output_dim : 512\n",
      "use_obj_knowledge : False\n",
      "obj_knowledge : \n",
      "use_ggnn_rel : False\n",
      "ggnn_rel_time_step_num : 3\n",
      "ggnn_rel_hidden_dim : 512\n",
      "ggnn_rel_output_dim : 512\n",
      "use_rel_knowledge : False\n",
      "rel_knowledge : \n",
      "tb_log_dir : summaries/kern_predcls/exp_045_rep\n",
      "save_rel_recall : \n"
     ]
    }
   ],
   "source": [
    "conf = ModelConfig(f'''\n",
    "-m predcls -p 1000 -clip 5 \n",
    "-tb_log_dir summaries/kern_predcls/{exp_name} \n",
    "-save_dir checkpoints/kern_predcls/{exp_name}\n",
    "-ckpt checkpoints/vgdet/vgrel-12.tar \n",
    "-val_size 5000 \n",
    "-adam \n",
    "-b 3\n",
    "-ngpu 1\n",
    "-lr 1e-4 \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.MODEL.CONF_MAT_FREQ_TRAIN = '../../../vgmeta/conf_mat_freq_train.npy'\n",
    "conf.MODEL.LRGA.USE_LRGA = False\n",
    "conf.MODEL.USE_ONTOLOGICAL_ADJUSTMENT = True\n",
    "conf.MODEL.NORMALIZE_EOA = True\n",
    "conf.num_workers = 9\n",
    "# conf.MODEL.LRGA.K = 50\n",
    "# conf.MODEL.LRGA.DROPOUT = 0.5\n",
    "# conf.MODEL.GN.NUM_GROUPS = 1024//8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/zhanwen/kangaroo/eoa_preds_2_norm_bpl_no_sa_20220326_legion/ipynb/train_predcls'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader using BPL\n",
      "Dataloader using BPL\n",
      "Dataloader using BPL\n"
     ]
    }
   ],
   "source": [
    "train, val, _ = VG.splits(num_val_im=conf.val_size, filter_duplicate_rels=True,\n",
    "                          use_proposals=conf.use_proposals,\n",
    "                          filter_non_overlap=conf.mode == 'sgdet', with_clean_classifier=True, get_state=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_to_predicates = train.ind_to_predicates # ind_to_predicates[0] means no relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = VGDataLoader.splits(train, val, mode='rel',\n",
    "                                               batch_size=conf.batch_size,\n",
    "                                               num_workers=conf.num_workers,\n",
    "                                               num_gpus=conf.num_gpus,\n",
    "                                               pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_ggnn_10: using use_ontological_adjustment\n",
      "EOA-N: Used adj_normalize\n",
      "!!!!!!!!!With Confusion Matrix Channel!!!!!\n",
      "SA: Used adj_normalize\n"
     ]
    }
   ],
   "source": [
    "detector = KERN(classes=train.ind_to_classes, rel_classes=train.ind_to_predicates,\n",
    "                num_gpus=conf.num_gpus, mode=conf.mode, require_overlap_det=True,\n",
    "                use_resnet=conf.use_resnet, use_proposals=conf.use_proposals, pooling_dim=conf.pooling_dim,\n",
    "                ggnn_rel_time_step_num=3, ggnn_rel_hidden_dim=1024, ggnn_rel_output_dim=None,\n",
    "                graph_path=os.path.join(codebase, '/home/zhanwen/gbnet/graphs/005/edge_dict_all_plus_wikidata_177_20220208.pkl'), \n",
    "                emb_path=os.path.join(codebase, '/home/zhanwen/gbnet/graphs/001/emb_mtx_wiki_51.pkl'), \n",
    "                rel_counts_path=os.path.join(codebase, 'graphs/001/pred_counts.pkl'), \n",
    "                use_knowledge=True, use_embedding=True, refine_obj_cls=False,\n",
    "                class_volume=1.0, with_clean_classifier=True, with_transfer=True, sa=True, config=conf,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the detector\n",
    "for n, param in detector.detector.named_parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 449.4M total parameters \n",
      " ----- \n",
      " \n",
      "detector.roi_fmap.0.weight                        : [4096,25088]    (102760448) (    )\n",
      "roi_fmap.1.0.weight                               : [4096,25088]    (102760448) (grad)\n",
      "roi_fmap_obj.0.weight                             : [4096,25088]    (102760448) (grad)\n",
      "detector.roi_fmap.3.weight                        : [4096,4096]     (16777216) (    )\n",
      "roi_fmap.1.3.weight                               : [4096,4096]     (16777216) (grad)\n",
      "roi_fmap_obj.3.weight                             : [4096,4096]     (16777216) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_ent.model.0.linear.weight: [3328,3328]     (11075584) (grad)\n",
      "ggnn_rel_reason.obj_proj.weight                   : [1024,4096]     ( 4194304) (grad)\n",
      "ggnn_rel_reason.rel_proj.weight                   : [1024,4096]     ( 4194304) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_pred.model.0.linear.weight: [2048,2048]     ( 4194304) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_ent.model.2.linear.weight: [1024,3328]     ( 3407872) (grad)\n",
      "detector.bbox_fc.weight                           : [708,4096]      ( 2899968) (    )\n",
      "detector.features.19.weight                       : [512,512,3,3]   ( 2359296) (    )\n",
      "detector.features.21.weight                       : [512,512,3,3]   ( 2359296) (    )\n",
      "detector.features.24.weight                       : [512,512,3,3]   ( 2359296) (    )\n",
      "detector.features.26.weight                       : [512,512,3,3]   ( 2359296) (    )\n",
      "detector.features.28.weight                       : [512,512,3,3]   ( 2359296) (    )\n",
      "detector.rpn_head.conv.0.weight                   : [512,512,3,3]   ( 2359296) (    )\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_pred.model.2.linear.weight: [1024,2048]     ( 2097152) (grad)\n",
      "detector.features.17.weight                       : [512,256,3,3]   ( 1179648) (    )\n",
      "union_boxes.conv.4.weight                         : [512,256,3,3]   ( 1179648) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_ont_ent.weight      : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_ont_ent.weight      : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_ont_ent.weight      : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_ont_ent.weight      : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_ont_ent.weight      : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_ont_ent.weight      : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_ont_pred.weight     : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_ont_pred.weight     : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_ont_pred.weight     : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_ont_pred.weight     : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_ont_pred.weight     : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_ont_pred.weight     : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_img_ent.weight      : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_img_ent.weight      : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_img_ent.weight      : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_img_ent.weight      : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_img_ent.weight      : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_img_ent.weight      : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_img_pred.weight     : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_img_pred.weight     : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_img_pred.weight     : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_img_pred.weight     : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_img_pred.weight     : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_img_pred.weight     : [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_img_pred.model.0.linear.weight: [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_img_pred.model.2.linear.weight: [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_ont_pred.model.0.linear.weight: [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_ont_pred.model.2.linear.weight: [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_img_pred_clean.model.0.linear.weight: [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_img_pred_clean.model.2.linear.weight: [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_ont_pred_clean.model.0.linear.weight: [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_ont_pred_clean.model.2.linear.weight: [1024,1024]     ( 1048576) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_ent.model.2.linear.weight: [1024,768]      (  786432) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_pred.model.2.linear.weight: [1024,768]      (  786432) (grad)\n",
      "detector.score_fc.weight                          : [177,4096]      (  724992) (    )\n",
      "detector.features.12.weight                       : [256,256,3,3]   (  589824) (    )\n",
      "detector.features.14.weight                       : [256,256,3,3]   (  589824) (    )\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_ent.model.0.linear.weight: [768,768]       (  589824) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_pred.model.0.linear.weight: [768,768]       (  589824) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_ent.model.0.linear.weight: [512,1024]      (  524288) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_pred.model.0.linear.weight: [512,1024]      (  524288) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_ent.model.0.linear.weight: [512,1024]      (  524288) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_pred.model.0.linear.weight: [512,1024]      (  524288) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_init_ont_ent.weight       : [1024,300]      (  307200) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_init_ont_pred.weight      : [1024,300]      (  307200) (grad)\n",
      "detector.features.10.weight                       : [256,128,3,3]   (  294912) (    )\n",
      "detector.features.7.weight                        : [128,128,3,3]   (  147456) (    )\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_ent.model.2.linear.weight: [256,512]       (  131072) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_pred.model.2.linear.weight: [256,512]       (  131072) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_ent.model.2.linear.weight: [256,512]       (  131072) (grad)\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_pred.model.2.linear.weight: [256,512]       (  131072) (grad)\n",
      "detector.features.5.weight                        : [128,64,3,3]    (   73728) (    )\n",
      "detector.rpn_head.conv.2.weight                   : [120,512,1,1]   (   61440) (    )\n",
      "detector.features.2.weight                        : [64,64,3,3]     (   36864) (    )\n",
      "union_boxes.conv.0.weight                         : [256,2,7,7]     (   25088) (grad)\n",
      "detector.features.0.weight                        : [64,3,3,3]      (    1728) (    )\n",
      "union_boxes.conv.6.weight                         : [512]           (     512) (grad)\n",
      "union_boxes.conv.2.weight                         : [256]           (     256) (grad)\n"
     ]
    }
   ],
   "source": [
    "print(print_para(detector), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex.optimizers import FusedAdam, FusedSGD\n",
    "def get_optim(lr):\n",
    "    # Lower the learning rate on the VGG fully connected layers by 1/10th. It's a hack, but it helps\n",
    "    # stabilize the models.\n",
    "    fc_params = [p for n,p in detector.named_parameters() if (n.startswith('roi_fmap') or 'clean' in n) and p.requires_grad]\n",
    "    non_fc_params = [p for n,p in detector.named_parameters() if not (n.startswith('roi_fmap') or 'clean' in n) and p.requires_grad]\n",
    "    params = [{'params': fc_params, 'lr': lr / 10.0}, {'params': non_fc_params}]\n",
    "    # params = [p for n,p in detector.named_parameters() if p.requires_grad]\n",
    "\n",
    "    if conf.adam:\n",
    "        optimizer = FusedAdam(params, weight_decay=conf.adamwd, lr=lr, eps=1e-3)\n",
    "    else:\n",
    "        optimizer = FusedSGD(params, weight_decay=conf.l2, lr=lr, momentum=0.9)\n",
    "\n",
    "    # scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3, factor=0.1,\n",
    "    #                               verbose=True, threshold=0.0001, threshold_mode='abs', cooldown=1)\n",
    "    return optimizer #, scheduler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(conf.ckpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EVERYTHING\n",
      "We couldn't find ggnn_rel_reason.ggnn.fc_output_proj_img_pred_clean.model.2.linear.bias,ggnn_rel_reason.ggnn.fc_output_proj_ont_pred_clean.model.2.linear.weight,ggnn_rel_reason.ggnn.fc_output_proj_img_pred_clean.model.0.linear.bias,ggnn_rel_reason.ggnn.fc_output_proj_img_pred_clean.model.0.linear.weight,ggnn_rel_reason.ggnn.fc_output_proj_ont_pred_clean.model.0.linear.bias,ggnn_rel_reason.ggnn.fc_output_proj_ont_pred_clean.model.2.linear.bias,ggnn_rel_reason.ggnn.fc_output_proj_img_pred_clean.model.2.linear.weight,ggnn_rel_reason.ggnn.fc_output_proj_ont_pred_clean.model.0.linear.weight\n"
     ]
    }
   ],
   "source": [
    "if conf.ckpt.split('-')[-2].split('/')[-1] == 'vgrel':\n",
    "    print(\"Loading EVERYTHING\")\n",
    "    start_epoch = ckpt['epoch']\n",
    "\n",
    "    if not optimistic_restore(detector, ckpt['state_dict']):\n",
    "        start_epoch = -1\n",
    "        # optimistic_restore(detector.detector, torch.load('checkpoints/vgdet/vg-28.tar')['state_dict'])\n",
    "else:\n",
    "    start_epoch = -1\n",
    "    optimistic_restore(detector.detector, ckpt['state_dict'])\n",
    "\n",
    "    detector.roi_fmap[1][0].weight.data.copy_(ckpt['state_dict']['roi_fmap.0.weight'])\n",
    "    detector.roi_fmap[1][3].weight.data.copy_(ckpt['state_dict']['roi_fmap.3.weight'])\n",
    "    detector.roi_fmap[1][0].bias.data.copy_(ckpt['state_dict']['roi_fmap.0.bias'])\n",
    "    detector.roi_fmap[1][3].bias.data.copy_(ckpt['state_dict']['roi_fmap.3.bias'])\n",
    "\n",
    "    detector.roi_fmap_obj[0].weight.data.copy_(ckpt['state_dict']['roi_fmap.0.weight'])\n",
    "    detector.roi_fmap_obj[3].weight.data.copy_(ckpt['state_dict']['roi_fmap.3.weight'])\n",
    "    detector.roi_fmap_obj[0].bias.data.copy_(ckpt['state_dict']['roi_fmap.0.bias'])\n",
    "    detector.roi_fmap_obj[3].bias.data.copy_(ckpt['state_dict']['roi_fmap.3.bias'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.cuda();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time as time_time\n",
    "def train_epoch(epoch_num):\n",
    "    detector.train()\n",
    "    tr = []\n",
    "    start = time_time()\n",
    "    prog_bar = tqdm(enumerate(train_loader), total=int(len(train)/train_loader.batch_size))\n",
    "    for b, batch in prog_bar:\n",
    "        result, loss_dict = train_batch(batch, verbose=b % (conf.print_interval*10) == 0)\n",
    "        tr.append(loss_dict)\n",
    "        '''\n",
    "        if b % 100 == 0:\n",
    "            print(loss_pd)\n",
    "            gt = result.rel_labels[:,3].data.cpu().numpy()\n",
    "            out = result.rel_dists.data.cpu().numpy()\n",
    "            ind = np.where(gt)[0]\n",
    "            print(gt[ind])\n",
    "            print(np.argmax(out[ind], 1))\n",
    "            print(np.argmax(out[ind, 1:], 1) + 1)\n",
    "        '''\n",
    "\n",
    "        if b % conf.print_interval == 0 and b >= conf.print_interval:\n",
    "#             mn = pd.DataFrame([pd.Series(dicty) for dicty in tr[-conf.print_interval:]]).mean(1)\n",
    "            mn = pd.DataFrame(tr[-conf.print_interval:]).mean(axis=0)\n",
    "            time_per_batch = (time_time() - start) / conf.print_interval\n",
    "            write(\"\\ne{:2d}b{:5d}/{:5d} {:.3f}s/batch, {:.1f}m/epoch\".format(\n",
    "                epoch_num, b, len(train_loader), time_per_batch, len(train_loader) * time_per_batch / 60))\n",
    "            write(mn.to_string())\n",
    "            write('-----------')\n",
    "            start = time_time()\n",
    "    return pd.DataFrame(tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "def train_batch(b, verbose=False):\n",
    "    \"\"\"\n",
    "    :param b: contains:\n",
    "          :param imgs: the image, [batch_size, 3, IM_SIZE, IM_SIZE]\n",
    "          :param all_anchors: [num_anchors, 4] the boxes of all anchors that we'll be using\n",
    "          :param all_anchor_inds: [num_anchors, 2] array of the indices into the concatenated\n",
    "                                  RPN feature vector that give us all_anchors,\n",
    "                                  each one (img_ind, fpn_idx)\n",
    "          :param im_sizes: a [batch_size, 4] numpy array of (h, w, scale, num_good_anchors) for each image.\n",
    "\n",
    "          :param num_anchors_per_img: int, number of anchors in total over the feature pyramid per img\n",
    "\n",
    "          Training parameters:\n",
    "          :param train_anchor_inds: a [num_train, 5] array of indices for the anchors that will\n",
    "                                    be used to compute the training loss (img_ind, fpn_idx)\n",
    "          :param gt_boxes: [num_gt, 4] GT boxes over the batch.\n",
    "          :param gt_classes: [num_gt, 2] gt boxes where each one is (img_id, class)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    with autocast():\n",
    "        result = detector[b]\n",
    "    #     losses = {}\n",
    "    #     losses['class_loss'] = float(detector.obj_loss(result))\n",
    "        loss_class = detector.obj_loss(result)\n",
    "    #     losses['rel_loss'] = float(detector.rel_loss(result))\n",
    "        loss_rel = detector.rel_loss(result)\n",
    "    #     loss = sum(losses.values())\n",
    "        loss = loss_class + loss_rel\n",
    "    #     loss.backward()\n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    clip_grad_norm(\n",
    "        [(n, p) for n, p in detector.named_parameters() if p.grad is not None],\n",
    "        max_norm=conf.clip, verbose=verbose, clip=True)\n",
    "#     losses['total'] = loss\n",
    "    optimizer.step()\n",
    "    return result, {\n",
    "          'loss_class': float(loss_class),\n",
    "          'loss_rel': float(loss_rel),\n",
    "          'loss_total': float(loss),\n",
    "    }\n",
    "#     loss_pd = pd.Series({x: y.detach() for x, y in losses.items()})\n",
    "#     return result, loss_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import no_grad as torch_no_grad\n",
    "from tqdm import tqdm\n",
    "\n",
    "def val_epoch():\n",
    "    detector.eval()\n",
    "    evaluator_list = [] # for calculating recall of each relationship except no relationship\n",
    "    evaluator_multiple_preds_list = []\n",
    "    for index, name in enumerate(ind_to_predicates):\n",
    "        if index == 0:\n",
    "            continue\n",
    "        evaluator_list.append((index, name, BasicSceneGraphEvaluator.all_modes()))\n",
    "        evaluator_multiple_preds_list.append((index, name, BasicSceneGraphEvaluator.all_modes(multiple_preds=True)))\n",
    "    evaluator = BasicSceneGraphEvaluator.all_modes() # for calculating recall\n",
    "    evaluator_multiple_preds = BasicSceneGraphEvaluator.all_modes(multiple_preds=True)\n",
    "    \n",
    "    prog_bar = tqdm(enumerate(val_loader), total=int(len(val)/val_loader.batch_size))\n",
    "    \n",
    "    with torch_no_grad():\n",
    "#         for i, data in prog_bar:\n",
    "        for val_b, batch in prog_bar:\n",
    "#         for val_b, batch in enumerate(val_loader):\n",
    "            val_batch(conf.num_gpus * val_b, batch, evaluator, evaluator_multiple_preds, evaluator_list, evaluator_multiple_preds_list)\n",
    "\n",
    "    recall = evaluator[conf.mode].print_stats()\n",
    "    recall_mp = evaluator_multiple_preds[conf.mode].print_stats()\n",
    "    \n",
    "    mean_recall = calculate_mR_from_evaluator_list(evaluator_list, conf.mode)\n",
    "    mean_recall_mp = calculate_mR_from_evaluator_list(evaluator_multiple_preds_list, conf.mode, multiple_preds=True)\n",
    "    \n",
    "    detector.train()\n",
    "    return recall, recall_mp, mean_recall, mean_recall_mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_batch(batch_num, b, evaluator, evaluator_multiple_preds, evaluator_list, evaluator_multiple_preds_list):\n",
    "    with autocast():\n",
    "        det_res = detector[b]\n",
    "    if conf.num_gpus == 1:\n",
    "        det_res = [det_res]\n",
    "\n",
    "    for i, (boxes_i, objs_i, obj_scores_i, rels_i, pred_scores_i) in enumerate(det_res):\n",
    "        gt_entry = {\n",
    "            'gt_classes': val.gt_classes[batch_num + i].copy(),\n",
    "            'gt_relations': val.relationships[batch_num + i].copy(),\n",
    "            'gt_boxes': val.gt_boxes[batch_num + i].copy(),\n",
    "        }\n",
    "        assert np.all(objs_i[rels_i[:, 0]] > 0) and np.all(objs_i[rels_i[:, 1]] > 0)\n",
    "\n",
    "        pred_entry = {\n",
    "            'pred_boxes': boxes_i * BOX_SCALE/IM_SCALE,\n",
    "            'pred_classes': objs_i,\n",
    "            'pred_rel_inds': rels_i,\n",
    "            'obj_scores': obj_scores_i,\n",
    "            'rel_scores': pred_scores_i,  # hack for now.\n",
    "        }\n",
    "\n",
    "        eval_entry(conf.mode, gt_entry, pred_entry, evaluator, evaluator_multiple_preds, \n",
    "                   evaluator_list, evaluator_multiple_preds_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.tb_log_dir is not None:\n",
    "    from tensorboardX import SummaryWriter\n",
    "    if not os.path.exists(conf.tb_log_dir):\n",
    "        os.makedirs(conf.tb_log_dir) \n",
    "    writer = SummaryWriter(log_dir=conf.tb_log_dir)\n",
    "    use_tb = True\n",
    "else:\n",
    "    use_tb = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "epoch=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                       | 1/6755 [00:00<1:30:15,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Total norm 7.256 clip coef 0.689-----------------\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_ont_pred_clean.model.2.linear.weight: 3.247, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_img_pred_clean.model.2.linear.weight: 3.123, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_ont_pred_clean.model.0.linear.weight: 3.111, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_img_pred_clean.model.0.linear.weight: 2.323, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_ont_pred.weight     : 1.781, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_img_pred.weight     : 1.347, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_img_pred_clean.model.2.linear.bias: 1.322, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_ont_pred.weight     : 1.268, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_ont_pred_clean.model.0.linear.bias: 1.161, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_ont_pred.bias       : 1.031, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_ont_pred.bias       : 1.031, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_init_ont_pred.weight      : 0.980, (torch.Size([1024, 300]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_img_pred.weight     : 0.817, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_img_pred_clean.model.0.linear.bias: 0.760, (torch.Size([1024]))\n",
      "roi_fmap.1.0.weight                               : 0.728, (torch.Size([4096, 25088]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_img_pred.bias       : 0.669, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_img_pred.bias       : 0.669, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_pred.model.2.linear.weight: 0.564, (torch.Size([1024, 2048]))\n",
      "roi_fmap.1.3.weight                               : 0.532, (torch.Size([4096, 4096]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_pred.model.2.linear.weight: 0.491, (torch.Size([1024, 768]))\n",
      "ggnn_rel_reason.rel_proj.weight                   : 0.433, (torch.Size([1024, 4096]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_pred.model.0.linear.weight: 0.420, (torch.Size([2048, 2048]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_pred.model.2.linear.bias: 0.382, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_pred.model.0.linear.bias: 0.269, (torch.Size([2048]))\n",
      "roi_fmap_obj.0.weight                             : 0.256, (torch.Size([4096, 25088]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_pred.model.0.linear.weight: 0.238, (torch.Size([768, 768]))\n",
      "ggnn_rel_reason.ggnn.fc_init_ont_pred.bias        : 0.220, (torch.Size([1024]))\n",
      "roi_fmap_obj.3.weight                             : 0.210, (torch.Size([4096, 4096]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_ont_pred.weight     : 0.207, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_img_pred.weight     : 0.142, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.rel_proj.bias                     : 0.115, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_pred.model.2.linear.bias: 0.110, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_ont_pred.weight     : 0.106, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_ent.model.2.linear.weight: 0.099, (torch.Size([256, 512]))\n",
      "roi_fmap.1.3.bias                                 : 0.083, (torch.Size([4096]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_pred.model.2.linear.weight: 0.083, (torch.Size([256, 512]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_ent.model.2.linear.weight: 0.081, (torch.Size([256, 512]))\n",
      "ggnn_rel_reason.obj_proj.weight                   : 0.071, (torch.Size([1024, 4096]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_pred.model.2.linear.weight: 0.069, (torch.Size([256, 512]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_img_pred.weight     : 0.067, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_ent.model.0.linear.weight: 0.064, (torch.Size([512, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_pred.model.0.linear.weight: 0.059, (torch.Size([512, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_ent.model.0.linear.weight: 0.057, (torch.Size([512, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_pred.model.0.linear.weight: 0.054, (torch.Size([512, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_img_ent.weight      : 0.046, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_ont_ent.weight      : 0.046, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_pred.model.0.linear.bias: 0.045, (torch.Size([768]))\n",
      "ggnn_rel_reason.ggnn.fc_init_ont_ent.weight       : 0.044, (torch.Size([1024, 300]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_ont_pred.bias       : 0.042, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_ont_pred.bias       : 0.042, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_ent.model.0.linear.weight: 0.040, (torch.Size([768, 768]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_ent.model.0.linear.weight: 0.040, (torch.Size([3328, 3328]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_pred.model.2.linear.bias: 0.039, (torch.Size([256]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_img_pred.weight     : 0.038, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_ent.model.2.linear.weight: 0.037, (torch.Size([1024, 768]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_ont_ent.weight      : 0.034, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_pred.model.2.linear.bias: 0.033, (torch.Size([256]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_pred.model.0.linear.bias: 0.033, (torch.Size([512]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_img_pred.bias       : 0.031, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_img_pred.bias       : 0.031, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_ent.model.2.linear.weight: 0.030, (torch.Size([1024, 3328]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_img_ent.weight      : 0.027, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_pred.model.0.linear.bias: 0.024, (torch.Size([512]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_ont_pred.weight     : 0.022, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_ont_pred.weight     : 0.020, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_ent.model.2.linear.bias: 0.018, (torch.Size([256]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_img_pred.weight     : 0.018, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_ent.model.2.linear.bias: 0.014, (torch.Size([256]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_ent.model.0.linear.bias: 0.013, (torch.Size([512]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_ont_pred.bias       : 0.011, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_ont_pred.bias       : 0.011, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_img_ent.bias        : 0.011, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_img_ent.bias        : 0.011, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_ent.model.0.linear.bias: 0.010, (torch.Size([512]))\n",
      "ggnn_rel_reason.ggnn.fc_init_ont_ent.bias         : 0.010, (torch.Size([1024]))\n",
      "ggnn_rel_reason.obj_proj.bias                     : 0.009, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_img_pred.bias       : 0.008, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_img_pred.bias       : 0.008, (torch.Size([1024]))\n",
      "roi_fmap.1.0.bias                                 : 0.007, (torch.Size([4096]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_ont_ent.weight      : 0.007, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_img_ent.weight      : 0.007, (torch.Size([1024, 1024]))\n",
      "roi_fmap_obj.3.bias                               : 0.007, (torch.Size([4096]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_ont_ent.weight      : 0.006, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_ent.model.2.linear.bias: 0.006, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_ent.model.0.linear.bias: 0.005, (torch.Size([768]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_img_ent.weight      : 0.005, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_ont_ent.bias        : 0.004, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_ont_ent.bias        : 0.004, (torch.Size([1024]))\n",
      "roi_fmap_obj.0.bias                               : 0.002, (torch.Size([4096]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_img_ent.weight      : 0.002, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_img_ent.weight      : 0.002, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_ent.model.2.linear.bias: 0.001, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_ont_ent.bias        : 0.001, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_ont_ent.bias        : 0.001, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_ont_ent.weight      : 0.001, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_ent.model.0.linear.bias: 0.001, (torch.Size([3328]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_img_ent.bias        : 0.001, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_img_ent.bias        : 0.001, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_ont_ent.weight      : 0.000, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_img_ent.bias        : 0.000, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_img_ent.bias        : 0.000, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_ont_pred_clean.model.2.linear.bias: 0.000, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_ont_ent.bias        : 0.000, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_ont_ent.bias        : 0.000, (torch.Size([1024]))\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████▋                                | 1002/6755 [02:56<16:54,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "e 0b 1000/ 6755 0.177s/batch, 19.9m/epoch\n",
      "loss_class    0.000000\n",
      "loss_rel      0.102519\n",
      "loss_total    0.102519\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████▎                          | 2002/6755 [05:51<14:16,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "e 0b 2000/ 6755 0.174s/batch, 19.6m/epoch\n",
      "loss_class    0.000000\n",
      "loss_rel      0.083669\n",
      "loss_total    0.083669\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████▉                     | 3002/6755 [08:45<10:33,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "e 0b 3000/ 6755 0.175s/batch, 19.7m/epoch\n",
      "loss_class    0.000000\n",
      "loss_rel      0.080781\n",
      "loss_total    0.080781\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|██████████████████████▌               | 4002/6755 [11:40<08:09,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "e 0b 4000/ 6755 0.174s/batch, 19.6m/epoch\n",
      "loss_class    0.000000\n",
      "loss_rel      0.077882\n",
      "loss_total    0.077882\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████▏         | 5002/6755 [14:32<04:48,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "e 0b 5000/ 6755 0.172s/batch, 19.4m/epoch\n",
      "loss_class    0.000000\n",
      "loss_rel      0.077006\n",
      "loss_total    0.077006\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████▏       | 5373/6755 [15:33<03:38,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|█████████████████████████████████▊    | 6002/6755 [17:16<02:01,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "e 0b 6000/ 6755 0.164s/batch, 18.5m/epoch\n",
      "loss_class    0.000000\n",
      "loss_rel      0.076003\n",
      "loss_total    0.076003\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 6755/6755 [19:20<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall 0: (0.083)\n",
      "loss_class    0.000000\n",
      "loss_rel      0.082767\n",
      "loss_total    0.082767\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 5000/5000 [03:24<00:00, 24.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================predcls  recall with constraint============================\n",
      "R@20: 0.342099\n",
      "R@50: 0.401877\n",
      "R@100: 0.415798\n",
      "======================predcls  recall without constraint============================\n",
      "R@20: 0.486882\n",
      "R@50: 0.682797\n",
      "R@100: 0.798635\n",
      "\n",
      "\n",
      "======================predcls  mean recall with constraint============================\n",
      "mR@20:  0.2863187368554035\n",
      "mR@50:  0.33131011335257154\n",
      "mR@100:  0.34514792088112095\n",
      "\n",
      "\n",
      "======================predcls  mean recall without constraint============================\n",
      "mR@20:  0.34885342444021317\n",
      "mR@50:  0.5188120486796873\n",
      "mR@100:  0.6200192716502937\n",
      "epoch=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/zhanwen/anaconda3/envs/gbnet/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/zhanwen/anaconda3/envs/gbnet/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  0%|                                         | 1/6755 [00:00<35:36,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Total norm 0.287 clip coef 17.432-----------------\n",
      "roi_fmap.1.0.weight                               : 0.168, (torch.Size([4096, 25088]))\n",
      "roi_fmap_obj.0.weight                             : 0.111, (torch.Size([4096, 25088]))\n",
      "roi_fmap_obj.3.weight                             : 0.096, (torch.Size([4096, 4096]))\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_img_pred_clean.model.0.linear.weight: 0.067, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_img_pred_clean.model.2.linear.weight: 0.061, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_ont_pred_clean.model.2.linear.weight: 0.060, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_ont_pred_clean.model.0.linear.weight: 0.059, (torch.Size([1024, 1024]))\n",
      "roi_fmap.1.3.weight                               : 0.053, (torch.Size([4096, 4096]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_img_pred.weight     : 0.048, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_pred.model.0.linear.weight: 0.041, (torch.Size([768, 768]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_pred.model.2.linear.weight: 0.041, (torch.Size([1024, 768]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_ont_pred.weight     : 0.029, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.obj_proj.weight                   : 0.027, (torch.Size([1024, 4096]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_ent.model.0.linear.weight: 0.026, (torch.Size([768, 768]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_img_pred.weight     : 0.025, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_ent.model.2.linear.weight: 0.025, (torch.Size([256, 512]))\n",
      "ggnn_rel_reason.rel_proj.weight                   : 0.020, (torch.Size([1024, 4096]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_ent.model.0.linear.weight: 0.020, (torch.Size([512, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_pred.model.0.linear.weight: 0.019, (torch.Size([2048, 2048]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_ent.model.2.linear.weight: 0.019, (torch.Size([1024, 768]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_img_ent.weight      : 0.018, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_ont_pred.weight     : 0.017, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_pred.model.2.linear.weight: 0.017, (torch.Size([1024, 2048]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_pred.model.2.linear.weight: 0.015, (torch.Size([256, 512]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_pred.model.0.linear.weight: 0.014, (torch.Size([512, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_pred.model.0.linear.weight: 0.014, (torch.Size([512, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_ent.model.0.linear.weight: 0.014, (torch.Size([3328, 3328]))\n",
      "ggnn_rel_reason.ggnn.fc_init_ont_pred.weight      : 0.013, (torch.Size([1024, 300]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_pred.model.2.linear.weight: 0.012, (torch.Size([256, 512]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_ent.model.2.linear.weight: 0.011, (torch.Size([256, 512]))\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_img_pred_clean.model.2.linear.bias: 0.011, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_init_ont_ent.weight       : 0.011, (torch.Size([1024, 300]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_ent.model.0.linear.weight: 0.011, (torch.Size([512, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_ont_pred.weight     : 0.011, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_img_pred_clean.model.0.linear.bias: 0.010, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_ont_ent.weight      : 0.009, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_img_pred.bias       : 0.009, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_img_pred.bias       : 0.009, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_img_ent.weight      : 0.009, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_ent.model.2.linear.weight: 0.008, (torch.Size([1024, 3328]))\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_ont_pred_clean.model.0.linear.bias: 0.007, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_img_pred.weight     : 0.006, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_img_pred.weight     : 0.006, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_ont_pred.bias       : 0.006, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_ont_pred.bias       : 0.006, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_pred.model.2.linear.bias: 0.005, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_ont_ent.weight      : 0.004, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_ont_pred.weight     : 0.004, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_pred.model.2.linear.bias: 0.004, (torch.Size([256]))\n",
      "ggnn_rel_reason.ggnn.fc_init_ont_pred.bias        : 0.004, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_pred.model.0.linear.bias: 0.004, (torch.Size([768]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_img_ent.weight      : 0.003, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_img_ent.weight      : 0.003, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_pred.model.2.linear.bias: 0.003, (torch.Size([256]))\n",
      "ggnn_rel_reason.rel_proj.bias                     : 0.003, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_img_pred.weight     : 0.003, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_pred.model.2.linear.bias: 0.003, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_pred.model.0.linear.bias: 0.003, (torch.Size([512]))\n",
      "roi_fmap.1.3.bias                                 : 0.003, (torch.Size([4096]))\n",
      "roi_fmap_obj.3.bias                               : 0.003, (torch.Size([4096]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_pred.model.0.linear.bias: 0.002, (torch.Size([512]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_pred.model.0.linear.bias: 0.002, (torch.Size([2048]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_ent.model.2.linear.bias: 0.002, (torch.Size([256]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_img_ent.model.0.linear.bias: 0.002, (torch.Size([512]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_img_ent.bias        : 0.002, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_img_ent.bias        : 0.002, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_init_ont_ent.bias         : 0.002, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_img_pred.weight     : 0.002, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_ont_ent.weight      : 0.002, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.obj_proj.bias                     : 0.002, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_ent.model.0.linear.bias: 0.002, (torch.Size([512]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_ent.model.2.linear.bias: 0.002, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_img_ent.model.0.linear.bias: 0.001, (torch.Size([768]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_send_ont_ent.model.2.linear.bias: 0.001, (torch.Size([256]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_ont_pred.weight     : 0.001, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_ont_ent.weight      : 0.001, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_ont_pred.weight     : 0.001, (torch.Size([1024, 1024]))\n",
      "roi_fmap.1.0.bias                                 : 0.001, (torch.Size([4096]))\n",
      "roi_fmap_obj.0.bias                               : 0.001, (torch.Size([4096]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_img_ent.weight      : 0.001, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_img_pred.bias       : 0.001, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_img_pred.bias       : 0.001, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_img_ent.weight      : 0.001, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_ont_pred.bias       : 0.001, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_ont_pred.bias       : 0.001, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_w_ont_ent.bias        : 0.001, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq5_u_ont_ent.bias        : 0.001, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_img_pred.bias       : 0.000, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_img_pred.bias       : 0.000, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_img_ent.bias        : 0.000, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_img_ent.bias        : 0.000, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_ont_ent.weight      : 0.000, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_ent.model.2.linear.bias: 0.000, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_ont_pred.bias       : 0.000, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_ont_pred.bias       : 0.000, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_mp_receive_ont_ent.model.0.linear.bias: 0.000, (torch.Size([3328]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_w_ont_ent.bias        : 0.000, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq3_u_ont_ent.bias        : 0.000, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_ont_ent.weight      : 0.000, (torch.Size([1024, 1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_img_ent.bias        : 0.000, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_img_ent.bias        : 0.000, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_output_proj_ont_pred_clean.model.2.linear.bias: 0.000, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_w_ont_ent.bias        : 0.000, (torch.Size([1024]))\n",
      "ggnn_rel_reason.ggnn.fc_eq4_u_ont_ent.bias        : 0.000, (torch.Size([1024]))\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████▋                                | 1002/6755 [02:44<16:02,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "e 1b 1000/ 6755 0.164s/batch, 18.5m/epoch\n",
      "loss_class    0.000000\n",
      "loss_rel      0.072656\n",
      "loss_total    0.072656\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████▏                            | 1643/6755 [04:29<13:30,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████▎                          | 2002/6755 [05:28<12:52,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "e 1b 2000/ 6755 0.164s/batch, 18.4m/epoch\n",
      "loss_class    0.000000\n",
      "loss_rel      0.073051\n",
      "loss_total    0.073051\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|██████████████▋                       | 2613/6755 [07:09<11:20,  6.09it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_475834/2837651654.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mrez\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mlosses_mean_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrez\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mlosses_mean_epoch_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses_mean_epoch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_475834/510469289.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch_num)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprog_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprog_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_interval\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         '''\n",
      "\u001b[0;32m/tmp/ipykernel_475834/2401028974.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(b, verbose)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#     loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     clip_grad_norm(\n\u001b[1;32m     35\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gbnet/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gbnet/lib/python3.8/site-packages/apex/amp/handle.py\u001b[0m in \u001b[0;36mscale_loss\u001b[0;34m(loss, optimizers, loss_id, model, delay_unscale, delay_overflow_check)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;31m# For future fused optimizers that enable sync-free dynamic loss scaling,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# should_skip will always be False.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mshould_skip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdelay_overflow_check\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloss_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_skip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gbnet/lib/python3.8/site-packages/apex/amp/scaler.py\u001b[0m in \u001b[0;36mupdate_scale\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# If the fused kernel is available, we only need one D2H memcopy and sync.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mLossScaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_fused_kernel\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_overflow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_overflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_overflow_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_overflow\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print(\"Training starts now!\")\n",
    "# import torch\n",
    "from warnings import warn\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "optimizer = get_optim(conf.lr * conf.num_gpus * conf.batch_size)\n",
    "detector, optimizer = amp.initialize(detector, optimizer, opt_level=\"O1\")\n",
    "\n",
    "start_epoch = 0\n",
    "end_epoch = 60\n",
    "for epoch in range(start_epoch, end_epoch):\n",
    "    write(f'epoch = {epoch}')\n",
    "    if epoch != 0 and epoch % 10 == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] /= 10\n",
    "    \n",
    "    rez = train_epoch(epoch)\n",
    "    losses_mean_epoch = rez.mean(axis=0)\n",
    "    losses_mean_epoch_class = losses_mean_epoch['loss_class']\n",
    "    losses_mean_epoch_rel = losses_mean_epoch['loss_rel']\n",
    "    losses_mean_epoch_total = losses_mean_epoch['loss_total']\n",
    "    write(\"overall{:2d}: ({:.3f})\\n{}\".format(epoch, losses_mean_epoch_total, losses_mean_epoch))\n",
    "\n",
    "    if use_tb:\n",
    "        writer.add_scalar('loss/rel_loss', losses_mean_epoch_rel, epoch)\n",
    "        writer.add_scalar('loss/class_loss', losses_mean_epoch_class, epoch)\n",
    "        writer.add_scalar('loss/total', losses_mean_epoch_total, epoch)\n",
    "\n",
    "    if conf.save_dir is not None:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'state_dict': detector.state_dict(), #{k:v for k,v in detector.state_dict().items() if not k.startswith('detector.')},\n",
    "            # 'optimizer': optimizer.state_dict(),\n",
    "        }, os.path.join(conf.save_dir, '{}-{}.tar'.format('vgrel', epoch)))\n",
    "\n",
    "    recall, recall_mp, mean_recall, mean_recall_mp = val_epoch()\n",
    "    if use_tb:\n",
    "        for key, value in recall.items():\n",
    "            writer.add_scalar('eval_' + conf.mode + '_with_constraint/' + key, value, epoch)\n",
    "        for key, value in recall_mp.items():\n",
    "            writer.add_scalar('eval_' + conf.mode + '_without_constraint/' + key, value, epoch)\n",
    "        for key, value in mean_recall.items():\n",
    "            writer.add_scalar('eval_' + conf.mode + '_with_constraint/mean ' + key, value, epoch)\n",
    "        for key, value in mean_recall_mp.items():\n",
    "            writer.add_scalar('eval_' + conf.mode + '_without_constraint/mean ' + key, value, epoch)\n",
    "\n",
    "        try:\n",
    "            writer.add_scalar('eval_' + conf.mode + 'loss_class', losses_mean_epoch_class, epoch)\n",
    "            writer.add_scalar('eval_' + conf.mode + 'loss_rel', losses_mean_epoch_rel, epoch)\n",
    "            writer.add_scalar('eval_' + conf.mode + 'loss_total', losses_mean_epoch_total, epoch)\n",
    "        except:\n",
    "            warn(f'Cannot add loss to writer')\n",
    "            \n",
    "    if epoch == 0:\n",
    "        train_loader.dataset.set_use_cache(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gbnet]",
   "language": "python",
   "name": "conda-env-gbnet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
